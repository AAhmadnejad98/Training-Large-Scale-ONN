{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Two-Pass Forward Propagation Training Approaches"
      ],
      "metadata": {
        "id": "NoBGcdDfRcUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "rQLvcQj5RjgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model (simple fully connected network for MNIST)\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "yq7fKMjRRkXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST data\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([transforms.ToTensor()])),\n",
        "    batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "HH1lkQbJRmfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "500u2JBFRoDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the random noise matrix F for error modulation\n",
        "def generate_random_matrix(input_size, output_size):\n",
        "    return torch.randn(input_size, output_size) * 0.01  # small random noise"
      ],
      "metadata": {
        "id": "vJyfnCNmRpKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F = generate_random_matrix(10, 28*28)  # F for input modulation (reshaped version of the modulation layer in paper)"
      ],
      "metadata": {
        "id": "fVJ7wq49RrAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Standard forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Compute the loss (error) and apply softmax to get probabilities\n",
        "        loss = criterion(output, target)\n",
        "        softmax_output = torch.softmax(output, dim=1)\n",
        "\n",
        "        # Compute the error signal\n",
        "        e = torch.nn.functional.one_hot(target, num_classes=10).float() - softmax_output\n",
        "\n",
        "        # Compute delta_x by modulating with the random matrix F\n",
        "        delta_x = torch.matmul(e, F).view(-1, 1, 28, 28)\n",
        "\n",
        "        # Modulated input: new input + delta_x\n",
        "        modulated_input = data + delta_x\n",
        "\n",
        "        # Forward pass with modulated input\n",
        "        modulated_output = model(modulated_input)\n",
        "\n",
        "        # Calculate the loss for modulated output\n",
        "        modulated_loss = criterion(modulated_output, target)\n",
        "\n",
        "        # Update the weights using modulated loss (no backpropagation, just gradient computation)\n",
        "        modulated_loss.backward()\n",
        "\n",
        "        # Perform manual gradient descent\n",
        "        with torch.no_grad():\n",
        "            for param in model.parameters():\n",
        "                param -= 0.01 * param.grad  # simple gradient descent with learning rate 0.01\n",
        "                param.grad.zero_()  # reset gradients\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "\n",
        "print('Training completed.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pHaKbCsRsQZ",
        "outputId": "60000a99-014c-4644-ca1f-7e4155fa9632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9636867642402649\n",
            "Epoch 2, Loss: 0.34817734360694885\n",
            "Epoch 3, Loss: 0.2073085606098175\n",
            "Epoch 4, Loss: 0.17624208331108093\n",
            "Epoch 5, Loss: 0.06440341472625732\n",
            "Epoch 6, Loss: 0.3194601535797119\n",
            "Epoch 7, Loss: 0.05183613300323486\n",
            "Epoch 8, Loss: 0.39785975217819214\n",
            "Epoch 9, Loss: 0.2508266866207123\n",
            "Epoch 10, Loss: 0.36374637484550476\n",
            "Training completed.\n"
          ]
        }
      ]
    }
  ]
}