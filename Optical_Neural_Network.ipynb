{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Training ONN with simple MZI model\n",
        "\n"
      ],
      "metadata": {
        "id": "_J2DwpT6R68k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iygD2i04P37e",
        "outputId": "c5984d63-526e-4643-fc2d-c4094940c2e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6813176870346069\n",
            "Epoch 2, Loss: 1.559756875038147\n",
            "Epoch 3, Loss: 1.1564719676971436\n",
            "Epoch 4, Loss: 0.7610306143760681\n",
            "Epoch 5, Loss: 0.8801256418228149\n",
            "Epoch 6, Loss: 0.8372388482093811\n",
            "Epoch 7, Loss: 0.6285146474838257\n",
            "Epoch 8, Loss: 0.6099349856376648\n",
            "Epoch 9, Loss: 0.4673929810523987\n",
            "Epoch 10, Loss: 0.35434451699256897\n",
            "Training completed.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "# Define a class for an MZI-based linear layer\n",
        "class MZILayer(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MZILayer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        # Randomly initialize phase shifts for the MZIs\n",
        "        self.theta = nn.Parameter(torch.rand(input_size, output_size) * 2 * np.pi)  # phase shifts\n",
        "        self.phi = nn.Parameter(torch.rand(input_size, output_size) * 2 * np.pi)    # phase shifts\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Simulate the MZI-based linear transformation\n",
        "        cos_theta = torch.cos(self.theta)\n",
        "        sin_theta = torch.sin(self.theta)\n",
        "        cos_phi = torch.cos(self.phi)\n",
        "        sin_phi = torch.sin(self.phi)\n",
        "\n",
        "        # Linear transformation using MZI-based operations\n",
        "        weight_matrix = cos_theta * cos_phi - sin_theta * sin_phi\n",
        "        mzi_output = torch.matmul(x, weight_matrix)  # input * weight matrix\n",
        "        return mzi_output\n",
        "\n",
        "# Define the optical neural network model using MZI layers\n",
        "class OpticalNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OpticalNN, self).__init__()\n",
        "        self.fc1 = MZILayer(28*28, 128)\n",
        "        self.fc2 = MZILayer(128, 64)\n",
        "        self.fc3 = MZILayer(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)  # Flatten input for MZI layer\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Load MNIST data\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([transforms.ToTensor()])),\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = OpticalNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the random noise matrix F for error modulation\n",
        "def generate_random_matrix(input_size, output_size):\n",
        "    return torch.randn(output_size, input_size) * 0.01  # F shape: (10, 784)\n",
        "\n",
        "F = generate_random_matrix(28*28, 10)  # F for input modulation with correct dimensions\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Standard forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Compute the loss (error) and apply softmax to get probabilities\n",
        "        loss = criterion(output, target)\n",
        "        softmax_output = torch.softmax(output, dim=1)\n",
        "\n",
        "        # Compute the error signal\n",
        "        e = torch.nn.functional.one_hot(target, num_classes=10).float() - softmax_output\n",
        "\n",
        "        # Compute delta_x by modulating with the random matrix F\n",
        "        delta_x = torch.matmul(e, F).view(-1, 1, 28, 28)  # Resulting shape: (batch_size, 784) -> reshaped\n",
        "\n",
        "        # Modulated input: new input + delta_x\n",
        "        modulated_input = data + delta_x\n",
        "\n",
        "        # Forward pass with modulated input\n",
        "        modulated_output = model(modulated_input)\n",
        "\n",
        "        # Calculate the loss for modulated output\n",
        "        modulated_loss = criterion(modulated_output, target)\n",
        "\n",
        "        # Update the MZI parameters using modulated loss (gradient-based update)\n",
        "        modulated_loss.backward()\n",
        "\n",
        "        # Perform manual gradient descent on the phase shifts (theta and phi)\n",
        "        with torch.no_grad():\n",
        "            for param in model.parameters():\n",
        "                param -= 0.01 * param.grad  # simple gradient descent with learning rate 0.01\n",
        "                param.grad.zero_()  # reset gradients\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "\n",
        "print('Training completed.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Large Scale ONN with simple MZI model"
      ],
      "metadata": {
        "id": "PHDtAwUUSADL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "# Define an MZI-based linear layer\n",
        "class MZILayer(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MZILayer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        # Initialize phase shifts for MZI simulation\n",
        "        self.theta = nn.Parameter(torch.rand(input_size, output_size) * 2 * np.pi)\n",
        "        self.phi = nn.Parameter(torch.rand(input_size, output_size) * 2 * np.pi)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cos_theta = torch.cos(self.theta)\n",
        "        sin_theta = torch.sin(self.theta)\n",
        "        cos_phi = torch.cos(self.phi)\n",
        "        sin_phi = torch.sin(self.phi)\n",
        "\n",
        "        weight_matrix = cos_theta * cos_phi - sin_theta * sin_phi\n",
        "        output = torch.matmul(x, weight_matrix)\n",
        "        return output\n",
        "\n",
        "# Define the model with MZI layers\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        # 28 MZI layers for rows\n",
        "        self.fc_row_1 = nn.ModuleList([MZILayer(28, 28) for _ in range(28)])\n",
        "        # 28 MZI layers for rows (second stage)\n",
        "        self.fc_row_2 = nn.ModuleList([MZILayer(28, 1) for _ in range(28)])\n",
        "        # Final layer to reduce to 10 classes\n",
        "        self.linear = MZILayer(28, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28, 28)\n",
        "        # Pass each row through its corresponding MZI layers\n",
        "        row = [self.relu(fc(x[:, :, i])) for i, fc in enumerate(self.fc_row_1)]\n",
        "        row = [self.relu(fc(row[i])) for i, fc in enumerate(self.fc_row_2)]\n",
        "        row = torch.cat(row, dim=1)\n",
        "        x = self.linear(row)\n",
        "        return x\n",
        "\n",
        "# Load MNIST data\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([transforms.ToTensor()])),\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = Model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the random noise matrix F for error modulation\n",
        "def generate_random_matrix(input_size, output_size):\n",
        "    return torch.randn(output_size, input_size) * 0.01\n",
        "\n",
        "F = generate_random_matrix(28*28, 10)  # F for input modulation\n",
        "\n",
        "# Training loop with PEPITA\n",
        "for epoch in range(10):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Standard forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Compute the loss (error) and apply softmax to get probabilities\n",
        "        loss = criterion(output, target)\n",
        "        softmax_output = torch.softmax(output, dim=1)\n",
        "\n",
        "        # Compute the error signal\n",
        "        e = torch.nn.functional.one_hot(target, num_classes=10).float() - softmax_output\n",
        "\n",
        "        # Compute delta_x by modulating with the random matrix F\n",
        "        delta_x = torch.matmul(e, F).view(-1, 1, 28, 28)\n",
        "\n",
        "        # Modulated input: new input + delta_x\n",
        "        modulated_input = data + delta_x\n",
        "\n",
        "        # Forward pass with modulated input\n",
        "        modulated_output = model(modulated_input)\n",
        "\n",
        "        # Calculate the loss for modulated output\n",
        "        modulated_loss = criterion(modulated_output, target)\n",
        "\n",
        "        # Update the MZI parameters using modulated loss (gradient-based update)\n",
        "        modulated_loss.backward()\n",
        "\n",
        "        # Perform manual gradient descent on the phase shifts (theta and phi)\n",
        "        with torch.no_grad():\n",
        "            for param in model.parameters():\n",
        "                param -= 0.01 * param.grad  # simple gradient descent with learning rate 0.01\n",
        "                param.grad.zero_()  # reset gradients\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "\n",
        "print('Training completed.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m-1_63BRWtl",
        "outputId": "30efed79-94db-459b-b7f9-08f1cfec0353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.5184834003448486\n",
            "Epoch 2, Loss: 1.4888489246368408\n",
            "Epoch 3, Loss: 1.5177112817764282\n",
            "Epoch 4, Loss: 1.304443120956421\n"
          ]
        }
      ]
    }
  ]
}